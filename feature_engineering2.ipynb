{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import seaborn as sns \n",
    "import scipy.stats as stats\n",
    "from sklearn.feature_selection import SelectKBest \n",
    "from sklearn.feature_selection import chi2 \n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('Compiled_descriptors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['MW', 'AMW', 'Sv', 'Mv', 'Me', 'Mp', 'Mi', 'GD', 'nTA', 'nBM',\n",
       "       ...\n",
       "       'ALOGP2', 'PDI', 'BLTF96', 'DLS_02', 'DLS_03', 'DLS_04', 'DLS_06',\n",
       "       'DLS_cons', 'LLS_01', 'LLS_02'],\n",
       "      dtype='object', length=737)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MW</th>\n",
       "      <th>AMW</th>\n",
       "      <th>Sv</th>\n",
       "      <th>Mv</th>\n",
       "      <th>Me</th>\n",
       "      <th>Mp</th>\n",
       "      <th>Mi</th>\n",
       "      <th>GD</th>\n",
       "      <th>nTA</th>\n",
       "      <th>nBM</th>\n",
       "      <th>...</th>\n",
       "      <th>PDI</th>\n",
       "      <th>BLTF96</th>\n",
       "      <th>DLS_02</th>\n",
       "      <th>DLS_03</th>\n",
       "      <th>DLS_04</th>\n",
       "      <th>DLS_06</th>\n",
       "      <th>DLS_cons</th>\n",
       "      <th>LLS_01</th>\n",
       "      <th>LLS_02</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>359.4</td>\n",
       "      <td>8.357</td>\n",
       "      <td>28.24</td>\n",
       "      <td>0.657</td>\n",
       "      <td>1.034</td>\n",
       "      <td>0.658</td>\n",
       "      <td>1.123</td>\n",
       "      <td>0.083</td>\n",
       "      <td>7.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.845</td>\n",
       "      <td>-2.11</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>368.4</td>\n",
       "      <td>7.839</td>\n",
       "      <td>30.56</td>\n",
       "      <td>0.650</td>\n",
       "      <td>1.017</td>\n",
       "      <td>0.667</td>\n",
       "      <td>1.115</td>\n",
       "      <td>0.080</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.880</td>\n",
       "      <td>-2.87</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>370.5</td>\n",
       "      <td>6.501</td>\n",
       "      <td>33.76</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.992</td>\n",
       "      <td>0.636</td>\n",
       "      <td>1.124</td>\n",
       "      <td>0.080</td>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.886</td>\n",
       "      <td>-5.32</td>\n",
       "      <td>0.83</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.88</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>249.3</td>\n",
       "      <td>7.334</td>\n",
       "      <td>22.23</td>\n",
       "      <td>0.654</td>\n",
       "      <td>0.988</td>\n",
       "      <td>0.694</td>\n",
       "      <td>1.117</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.018</td>\n",
       "      <td>-4.72</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.67</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>269.8</td>\n",
       "      <td>8.702</td>\n",
       "      <td>21.53</td>\n",
       "      <td>0.694</td>\n",
       "      <td>1.002</td>\n",
       "      <td>0.732</td>\n",
       "      <td>1.113</td>\n",
       "      <td>0.123</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.023</td>\n",
       "      <td>-4.95</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.83</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>304.3</td>\n",
       "      <td>8.949</td>\n",
       "      <td>23.16</td>\n",
       "      <td>0.681</td>\n",
       "      <td>1.047</td>\n",
       "      <td>0.669</td>\n",
       "      <td>1.116</td>\n",
       "      <td>0.104</td>\n",
       "      <td>6.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.841</td>\n",
       "      <td>-1.32</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>365.6</td>\n",
       "      <td>7.947</td>\n",
       "      <td>30.42</td>\n",
       "      <td>0.661</td>\n",
       "      <td>0.984</td>\n",
       "      <td>0.729</td>\n",
       "      <td>1.104</td>\n",
       "      <td>0.093</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.023</td>\n",
       "      <td>-4.75</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>265.4</td>\n",
       "      <td>6.805</td>\n",
       "      <td>24.28</td>\n",
       "      <td>0.623</td>\n",
       "      <td>0.984</td>\n",
       "      <td>0.669</td>\n",
       "      <td>1.123</td>\n",
       "      <td>0.116</td>\n",
       "      <td>4.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-3.84</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>292.3</td>\n",
       "      <td>8.119</td>\n",
       "      <td>21.45</td>\n",
       "      <td>0.596</td>\n",
       "      <td>1.056</td>\n",
       "      <td>0.583</td>\n",
       "      <td>1.155</td>\n",
       "      <td>0.100</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.769</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.75</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>198.3</td>\n",
       "      <td>6.838</td>\n",
       "      <td>18.20</td>\n",
       "      <td>0.628</td>\n",
       "      <td>0.983</td>\n",
       "      <td>0.675</td>\n",
       "      <td>1.120</td>\n",
       "      <td>0.162</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.052</td>\n",
       "      <td>-3.27</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.83</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 738 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        MW    AMW     Sv     Mv     Me     Mp     Mi     GD  nTA   nBM  ...  \\\n",
       "0    359.4  8.357  28.24  0.657  1.034  0.658  1.123  0.083  7.0  15.0  ...   \n",
       "1    368.4  7.839  30.56  0.650  1.017  0.667  1.115  0.080  6.0  16.0  ...   \n",
       "2    370.5  6.501  33.76  0.592  0.992  0.636  1.124  0.080  8.0  10.0  ...   \n",
       "3    249.3  7.334  22.23  0.654  0.988  0.694  1.117  0.123  0.0  17.0  ...   \n",
       "4    269.8  8.702  21.53  0.694  1.002  0.732  1.113  0.123  1.0  17.0  ...   \n",
       "..     ...    ...    ...    ...    ...    ...    ...    ...  ...   ...  ...   \n",
       "564  304.3  8.949  23.16  0.681  1.047  0.669  1.116  0.104  6.0  13.0  ...   \n",
       "565  365.6  7.947  30.42  0.661  0.984  0.729  1.104  0.093  2.0  15.0  ...   \n",
       "566  265.4  6.805  24.28  0.623  0.984  0.669  1.123  0.116  4.0  16.0  ...   \n",
       "567  292.3  8.119  21.45  0.596  1.056  0.583  1.155  0.100  8.0   4.0  ...   \n",
       "568  198.3  6.838  18.20  0.628  0.983  0.675  1.120  0.162  1.0  11.0  ...   \n",
       "\n",
       "       PDI  BLTF96  DLS_02  DLS_03  DLS_04  DLS_06  DLS_cons  LLS_01  LLS_02  \\\n",
       "0    0.845   -2.11    0.83    0.83     0.7    0.67      0.61    0.17    0.88   \n",
       "1    0.880   -2.87    1.00    1.00     0.6    1.00      0.80    0.33    1.00   \n",
       "2    0.886   -5.32    0.83    1.00     1.0    1.00      0.94    0.17    0.88   \n",
       "3    1.018   -4.72    1.00    1.00     0.6    1.00      0.87    0.67    1.00   \n",
       "4    1.023   -4.95    1.00    1.00     0.6    1.00      0.76    0.83    1.00   \n",
       "..     ...     ...     ...     ...     ...     ...       ...     ...     ...   \n",
       "564  0.841   -1.32    1.00    1.00     0.4    1.00      0.77    0.33    1.00   \n",
       "565  1.023   -4.75    1.00    1.00     0.8    1.00      0.90    0.33    1.00   \n",
       "566  1.000   -3.84    1.00    1.00     0.8    1.00      0.90    1.00    1.00   \n",
       "567  0.769   -0.24    0.67    0.83     0.9    0.67      0.72    0.33    0.75   \n",
       "568  1.052   -3.27    0.67    0.83     0.8    1.00      0.83    1.00    1.00   \n",
       "\n",
       "     Class  \n",
       "0        0  \n",
       "1        1  \n",
       "2        2  \n",
       "3        2  \n",
       "4        2  \n",
       "..     ...  \n",
       "564      2  \n",
       "565      1  \n",
       "566      0  \n",
       "567      2  \n",
       "568      0  \n",
       "\n",
       "[569 rows x 738 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df.drop('Class',axis=1)\n",
    "y=df['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar= MinMaxScaler((0,400))\n",
    "scaled_data=scalar.fit_transform(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df=pd.DataFrame(scaled_data, columns=df.columns[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['Class']=df['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MW</th>\n",
       "      <th>AMW</th>\n",
       "      <th>Sv</th>\n",
       "      <th>Mv</th>\n",
       "      <th>Me</th>\n",
       "      <th>Mp</th>\n",
       "      <th>Mi</th>\n",
       "      <th>GD</th>\n",
       "      <th>nTA</th>\n",
       "      <th>nBM</th>\n",
       "      <th>...</th>\n",
       "      <th>PDI</th>\n",
       "      <th>BLTF96</th>\n",
       "      <th>DLS_02</th>\n",
       "      <th>DLS_03</th>\n",
       "      <th>DLS_04</th>\n",
       "      <th>DLS_06</th>\n",
       "      <th>DLS_cons</th>\n",
       "      <th>LLS_01</th>\n",
       "      <th>LLS_02</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>67.712587</td>\n",
       "      <td>95.222930</td>\n",
       "      <td>85.575758</td>\n",
       "      <td>327.272727</td>\n",
       "      <td>218.867925</td>\n",
       "      <td>119.197708</td>\n",
       "      <td>172.093023</td>\n",
       "      <td>109.565217</td>\n",
       "      <td>155.555556</td>\n",
       "      <td>130.434783</td>\n",
       "      <td>...</td>\n",
       "      <td>302.055407</td>\n",
       "      <td>234.482759</td>\n",
       "      <td>332.0</td>\n",
       "      <td>298.507463</td>\n",
       "      <td>266.666667</td>\n",
       "      <td>240.963855</td>\n",
       "      <td>212.048193</td>\n",
       "      <td>68.0</td>\n",
       "      <td>323.809524</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>70.291568</td>\n",
       "      <td>76.893135</td>\n",
       "      <td>92.606061</td>\n",
       "      <td>323.785803</td>\n",
       "      <td>154.716981</td>\n",
       "      <td>129.512894</td>\n",
       "      <td>134.883721</td>\n",
       "      <td>104.347826</td>\n",
       "      <td>133.333333</td>\n",
       "      <td>139.130435</td>\n",
       "      <td>...</td>\n",
       "      <td>314.566577</td>\n",
       "      <td>195.657727</td>\n",
       "      <td>400.0</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>222.222222</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>303.614458</td>\n",
       "      <td>132.0</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70.893330</td>\n",
       "      <td>29.547063</td>\n",
       "      <td>102.303030</td>\n",
       "      <td>294.894147</td>\n",
       "      <td>60.377358</td>\n",
       "      <td>93.982808</td>\n",
       "      <td>176.744186</td>\n",
       "      <td>104.347826</td>\n",
       "      <td>177.777778</td>\n",
       "      <td>86.956522</td>\n",
       "      <td>...</td>\n",
       "      <td>316.711349</td>\n",
       "      <td>70.498084</td>\n",
       "      <td>332.0</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>371.084337</td>\n",
       "      <td>68.0</td>\n",
       "      <td>323.809524</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36.163049</td>\n",
       "      <td>59.023355</td>\n",
       "      <td>67.363636</td>\n",
       "      <td>325.778331</td>\n",
       "      <td>45.283019</td>\n",
       "      <td>160.458453</td>\n",
       "      <td>144.186047</td>\n",
       "      <td>179.130435</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>147.826087</td>\n",
       "      <td>...</td>\n",
       "      <td>363.896336</td>\n",
       "      <td>101.149425</td>\n",
       "      <td>400.0</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>222.222222</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>337.349398</td>\n",
       "      <td>268.0</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42.037395</td>\n",
       "      <td>107.430998</td>\n",
       "      <td>65.242424</td>\n",
       "      <td>345.703611</td>\n",
       "      <td>98.113208</td>\n",
       "      <td>204.011461</td>\n",
       "      <td>125.581395</td>\n",
       "      <td>179.130435</td>\n",
       "      <td>22.222222</td>\n",
       "      <td>147.826087</td>\n",
       "      <td>...</td>\n",
       "      <td>365.683646</td>\n",
       "      <td>89.399745</td>\n",
       "      <td>400.0</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>222.222222</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>284.337349</td>\n",
       "      <td>332.0</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>51.923490</td>\n",
       "      <td>116.171267</td>\n",
       "      <td>70.181818</td>\n",
       "      <td>339.227895</td>\n",
       "      <td>267.924528</td>\n",
       "      <td>131.805158</td>\n",
       "      <td>139.534884</td>\n",
       "      <td>146.086957</td>\n",
       "      <td>133.333333</td>\n",
       "      <td>113.043478</td>\n",
       "      <td>...</td>\n",
       "      <td>300.625559</td>\n",
       "      <td>274.840358</td>\n",
       "      <td>400.0</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>133.333333</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>289.156627</td>\n",
       "      <td>132.0</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>69.489218</td>\n",
       "      <td>80.714791</td>\n",
       "      <td>92.181818</td>\n",
       "      <td>329.265255</td>\n",
       "      <td>30.188679</td>\n",
       "      <td>200.573066</td>\n",
       "      <td>83.720930</td>\n",
       "      <td>126.956522</td>\n",
       "      <td>44.444444</td>\n",
       "      <td>130.434783</td>\n",
       "      <td>...</td>\n",
       "      <td>365.683646</td>\n",
       "      <td>99.616858</td>\n",
       "      <td>400.0</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>311.111111</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>351.807229</td>\n",
       "      <td>132.0</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>40.776560</td>\n",
       "      <td>40.304317</td>\n",
       "      <td>73.575758</td>\n",
       "      <td>310.336239</td>\n",
       "      <td>30.188679</td>\n",
       "      <td>131.805158</td>\n",
       "      <td>172.093023</td>\n",
       "      <td>166.956522</td>\n",
       "      <td>88.888889</td>\n",
       "      <td>139.130435</td>\n",
       "      <td>...</td>\n",
       "      <td>357.462020</td>\n",
       "      <td>146.104725</td>\n",
       "      <td>400.0</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>311.111111</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>351.807229</td>\n",
       "      <td>400.0</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>48.484848</td>\n",
       "      <td>86.801132</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>296.886675</td>\n",
       "      <td>301.886792</td>\n",
       "      <td>33.237822</td>\n",
       "      <td>320.930233</td>\n",
       "      <td>139.130435</td>\n",
       "      <td>177.777778</td>\n",
       "      <td>34.782609</td>\n",
       "      <td>...</td>\n",
       "      <td>274.888293</td>\n",
       "      <td>330.012771</td>\n",
       "      <td>268.0</td>\n",
       "      <td>298.507463</td>\n",
       "      <td>355.555556</td>\n",
       "      <td>240.963855</td>\n",
       "      <td>265.060241</td>\n",
       "      <td>132.0</td>\n",
       "      <td>241.269841</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>21.548822</td>\n",
       "      <td>41.472045</td>\n",
       "      <td>55.151515</td>\n",
       "      <td>312.826899</td>\n",
       "      <td>26.415094</td>\n",
       "      <td>138.681948</td>\n",
       "      <td>158.139535</td>\n",
       "      <td>246.956522</td>\n",
       "      <td>22.222222</td>\n",
       "      <td>95.652174</td>\n",
       "      <td>...</td>\n",
       "      <td>376.050045</td>\n",
       "      <td>175.223499</td>\n",
       "      <td>268.0</td>\n",
       "      <td>298.507463</td>\n",
       "      <td>311.111111</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>318.072289</td>\n",
       "      <td>400.0</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 738 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            MW         AMW          Sv          Mv          Me          Mp  \\\n",
       "0    67.712587   95.222930   85.575758  327.272727  218.867925  119.197708   \n",
       "1    70.291568   76.893135   92.606061  323.785803  154.716981  129.512894   \n",
       "2    70.893330   29.547063  102.303030  294.894147   60.377358   93.982808   \n",
       "3    36.163049   59.023355   67.363636  325.778331   45.283019  160.458453   \n",
       "4    42.037395  107.430998   65.242424  345.703611   98.113208  204.011461   \n",
       "..         ...         ...         ...         ...         ...         ...   \n",
       "564  51.923490  116.171267   70.181818  339.227895  267.924528  131.805158   \n",
       "565  69.489218   80.714791   92.181818  329.265255   30.188679  200.573066   \n",
       "566  40.776560   40.304317   73.575758  310.336239   30.188679  131.805158   \n",
       "567  48.484848   86.801132   65.000000  296.886675  301.886792   33.237822   \n",
       "568  21.548822   41.472045   55.151515  312.826899   26.415094  138.681948   \n",
       "\n",
       "             Mi          GD         nTA         nBM  ...         PDI  \\\n",
       "0    172.093023  109.565217  155.555556  130.434783  ...  302.055407   \n",
       "1    134.883721  104.347826  133.333333  139.130435  ...  314.566577   \n",
       "2    176.744186  104.347826  177.777778   86.956522  ...  316.711349   \n",
       "3    144.186047  179.130435    0.000000  147.826087  ...  363.896336   \n",
       "4    125.581395  179.130435   22.222222  147.826087  ...  365.683646   \n",
       "..          ...         ...         ...         ...  ...         ...   \n",
       "564  139.534884  146.086957  133.333333  113.043478  ...  300.625559   \n",
       "565   83.720930  126.956522   44.444444  130.434783  ...  365.683646   \n",
       "566  172.093023  166.956522   88.888889  139.130435  ...  357.462020   \n",
       "567  320.930233  139.130435  177.777778   34.782609  ...  274.888293   \n",
       "568  158.139535  246.956522   22.222222   95.652174  ...  376.050045   \n",
       "\n",
       "         BLTF96  DLS_02      DLS_03      DLS_04      DLS_06    DLS_cons  \\\n",
       "0    234.482759   332.0  298.507463  266.666667  240.963855  212.048193   \n",
       "1    195.657727   400.0  400.000000  222.222222  400.000000  303.614458   \n",
       "2     70.498084   332.0  400.000000  400.000000  400.000000  371.084337   \n",
       "3    101.149425   400.0  400.000000  222.222222  400.000000  337.349398   \n",
       "4     89.399745   400.0  400.000000  222.222222  400.000000  284.337349   \n",
       "..          ...     ...         ...         ...         ...         ...   \n",
       "564  274.840358   400.0  400.000000  133.333333  400.000000  289.156627   \n",
       "565   99.616858   400.0  400.000000  311.111111  400.000000  351.807229   \n",
       "566  146.104725   400.0  400.000000  311.111111  400.000000  351.807229   \n",
       "567  330.012771   268.0  298.507463  355.555556  240.963855  265.060241   \n",
       "568  175.223499   268.0  298.507463  311.111111  400.000000  318.072289   \n",
       "\n",
       "     LLS_01      LLS_02  Class  \n",
       "0      68.0  323.809524      0  \n",
       "1     132.0  400.000000      1  \n",
       "2      68.0  323.809524      2  \n",
       "3     268.0  400.000000      2  \n",
       "4     332.0  400.000000      2  \n",
       "..      ...         ...    ...  \n",
       "564   132.0  400.000000      2  \n",
       "565   132.0  400.000000      1  \n",
       "566   400.0  400.000000      0  \n",
       "567   132.0  241.269841      2  \n",
       "568   400.0  400.000000      0  \n",
       "\n",
       "[569 rows x 738 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trf=new_df.drop('Class',axis=1)\n",
    "y_trf=new_df['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=30\n",
    "features_chi2=SelectKBest(chi2, k=k)\n",
    "x_new=features_chi2.fit_transform(x_trf,y_trf)\n",
    "selected_features=df.columns[:-1][features_chi2.get_support()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nF', 'nX', 'P_VSA_ppp_hal', 'SM03_EA(dm)', 'nR=Cs', 'nR=Ct', 'C-005', 'C-006', 'C-016', 'H-053', 'O-060', 'SdsCH', 'CATS2D_04_DD', 'CATS2D_04_DA', 'CATS2D_05_DA', 'CATS2D_07_DL', 'CATS2D_03_PL', 'T(O..F)', 'T(F..F)', 'F03[C-S]', 'F04[N-Cl]', 'F05[C-S]', 'F05[N-N]', 'F05[N-F]', 'F06[C-F]', 'F07[C-F]', 'F07[C-Br]', 'F09[C-F]', 'F09[N-F]', 'F10[C-F]']\n"
     ]
    }
   ],
   "source": [
    "print(selected_features.to_list())\n",
    "final_df=df[selected_features.to_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akshat\\AppData\\Local\\Temp\\ipykernel_8672\\3084608001.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final_df['Class']=df['Class']\n"
     ]
    }
   ],
   "source": [
    "final_df['Class']=df['Class']\n",
    "m=final_df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_final=new_df.drop(\"Class\",axis=1)\n",
    "y_final=new_df['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class            1.000000\n",
       "C-005            0.311539\n",
       "VE3sign_D/Dt     0.293905\n",
       "CATS2D_04_DA     0.293005\n",
       "C-016            0.272676\n",
       "MATS2p           0.268890\n",
       "P_VSA_e_2        0.248003\n",
       "SdsCH            0.246290\n",
       "VE3sign_Dz(p)    0.246287\n",
       "P_VSA_i_2        0.239823\n",
       "Name: Class, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(new_df.corr(method='spearman')['Class']).sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier,VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1=RandomForestClassifier()\n",
    "clf2=LogisticRegression()\n",
    "clf3=GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_clf_hard = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('RF', clf1),  # Include the first classifier (Logistic Regression)\n",
    "          # Include the second classifier (Random Forest)\n",
    "        ('GBC', clf3),  # Include the third classifier (Naive Bayes)\n",
    "    ],\n",
    "    voting='hard'  # Specify hard voting, where the majority class prediction is chosen\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(cross_val_score(clf3,x_final,y_final,cv=20,scoring='accuracy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nF</th>\n",
       "      <th>nX</th>\n",
       "      <th>P_VSA_ppp_hal</th>\n",
       "      <th>SM03_EA(dm)</th>\n",
       "      <th>nR=Cs</th>\n",
       "      <th>nR=Ct</th>\n",
       "      <th>C-005</th>\n",
       "      <th>C-006</th>\n",
       "      <th>C-016</th>\n",
       "      <th>H-053</th>\n",
       "      <th>...</th>\n",
       "      <th>F05[C-S]</th>\n",
       "      <th>F05[N-N]</th>\n",
       "      <th>F05[N-F]</th>\n",
       "      <th>F06[C-F]</th>\n",
       "      <th>F07[C-F]</th>\n",
       "      <th>F07[C-Br]</th>\n",
       "      <th>F09[C-F]</th>\n",
       "      <th>F09[N-F]</th>\n",
       "      <th>F10[C-F]</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.742608</td>\n",
       "      <td>133.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>133.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>400.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>133.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>333.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>266.666667</td>\n",
       "      <td>133.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>266.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>58.770180</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>53.333333</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>72.727273</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.356793</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>266.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>258.879013</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      nF         nX  P_VSA_ppp_hal  SM03_EA(dm)       nR=Cs       nR=Ct  \\\n",
       "0    0.0   0.000000            0.0   162.742608  133.333333    0.000000   \n",
       "1    0.0   0.000000            0.0     0.000000  400.000000    0.000000   \n",
       "2    0.0   0.000000            0.0     0.000000  266.666667  133.333333   \n",
       "3    0.0   0.000000            0.0     0.000000    0.000000    0.000000   \n",
       "4    0.0  66.666667          100.0     0.000000    0.000000    0.000000   \n",
       "..   ...        ...            ...          ...         ...         ...   \n",
       "564  0.0   0.000000            0.0     0.000000    0.000000    0.000000   \n",
       "565  0.0   0.000000            0.0    58.770180  200.000000    0.000000   \n",
       "566  0.0   0.000000            0.0    41.356793    0.000000    0.000000   \n",
       "567  0.0   0.000000            0.0   258.879013    0.000000    0.000000   \n",
       "568  0.0   0.000000            0.0     0.000000    0.000000    0.000000   \n",
       "\n",
       "          C-005       C-006       C-016  H-053  ...   F05[C-S]  F05[N-N]  \\\n",
       "0      0.000000    0.000000  133.333333    0.0  ...   0.000000       0.0   \n",
       "1    133.333333    0.000000  333.333333    0.0  ...   0.000000       0.0   \n",
       "2      0.000000    0.000000  266.666667    0.0  ...   0.000000       0.0   \n",
       "3      0.000000   26.666667    0.000000    0.0  ...   0.000000       0.0   \n",
       "4      0.000000   26.666667    0.000000    0.0  ...   0.000000       0.0   \n",
       "..          ...         ...         ...    ...  ...        ...       ...   \n",
       "564    0.000000    0.000000    0.000000    0.0  ...   0.000000       0.0   \n",
       "565    0.000000   53.333333  200.000000    0.0  ...  72.727273       0.0   \n",
       "566  266.666667    0.000000    0.000000    0.0  ...   0.000000       0.0   \n",
       "567    0.000000  160.000000    0.000000    0.0  ...   0.000000       0.0   \n",
       "568    0.000000    0.000000    0.000000    0.0  ...   0.000000       0.0   \n",
       "\n",
       "     F05[N-F]  F06[C-F]  F07[C-F]  F07[C-Br]  F09[C-F]  F09[N-F]  F10[C-F]  \\\n",
       "0         0.0       0.0       0.0        0.0       0.0       0.0       0.0   \n",
       "1         0.0       0.0       0.0        0.0       0.0       0.0       0.0   \n",
       "2         0.0       0.0       0.0        0.0       0.0       0.0       0.0   \n",
       "3         0.0       0.0       0.0        0.0       0.0       0.0       0.0   \n",
       "4         0.0       0.0       0.0        0.0       0.0       0.0       0.0   \n",
       "..        ...       ...       ...        ...       ...       ...       ...   \n",
       "564       0.0       0.0       0.0        0.0       0.0       0.0       0.0   \n",
       "565       0.0       0.0       0.0        0.0       0.0       0.0       0.0   \n",
       "566       0.0       0.0       0.0        0.0       0.0       0.0       0.0   \n",
       "567       0.0       0.0       0.0        0.0       0.0       0.0       0.0   \n",
       "568       0.0       0.0       0.0        0.0       0.0       0.0       0.0   \n",
       "\n",
       "     Class  \n",
       "0        0  \n",
       "1        1  \n",
       "2        2  \n",
       "3        2  \n",
       "4        2  \n",
       "..     ...  \n",
       "564      2  \n",
       "565      1  \n",
       "566      0  \n",
       "567      2  \n",
       "568      0  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df[final_df.columns.to_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_df=df[m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nF</th>\n",
       "      <th>nX</th>\n",
       "      <th>P_VSA_ppp_hal</th>\n",
       "      <th>SM03_EA(dm)</th>\n",
       "      <th>nR=Cs</th>\n",
       "      <th>nR=Ct</th>\n",
       "      <th>C-005</th>\n",
       "      <th>C-006</th>\n",
       "      <th>C-016</th>\n",
       "      <th>H-053</th>\n",
       "      <th>...</th>\n",
       "      <th>F05[C-S]</th>\n",
       "      <th>F05[N-N]</th>\n",
       "      <th>F05[N-F]</th>\n",
       "      <th>F06[C-F]</th>\n",
       "      <th>F07[C-F]</th>\n",
       "      <th>F07[C-Br]</th>\n",
       "      <th>F09[C-F]</th>\n",
       "      <th>F09[N-F]</th>\n",
       "      <th>F10[C-F]</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.243</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39.15</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.810</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.568</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      nF   nX  P_VSA_ppp_hal  SM03_EA(dm)  nR=Cs  nR=Ct  C-005  C-006  C-016  \\\n",
       "0    0.0  0.0           0.00        2.243    2.0    0.0    0.0    0.0    2.0   \n",
       "1    0.0  0.0           0.00        0.000    6.0    0.0    2.0    0.0    5.0   \n",
       "2    0.0  0.0           0.00        0.000    4.0    2.0    0.0    0.0    4.0   \n",
       "3    0.0  0.0           0.00        0.000    0.0    0.0    0.0    1.0    0.0   \n",
       "4    0.0  1.0          39.15        0.000    0.0    0.0    0.0    1.0    0.0   \n",
       "..   ...  ...            ...          ...    ...    ...    ...    ...    ...   \n",
       "564  0.0  0.0           0.00        0.000    0.0    0.0    0.0    0.0    0.0   \n",
       "565  0.0  0.0           0.00        0.810    3.0    0.0    0.0    2.0    3.0   \n",
       "566  0.0  0.0           0.00        0.570    0.0    0.0    4.0    0.0    0.0   \n",
       "567  0.0  0.0           0.00        3.568    0.0    0.0    0.0    6.0    0.0   \n",
       "568  0.0  0.0           0.00        0.000    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "     H-053  ...  F05[C-S]  F05[N-N]  F05[N-F]  F06[C-F]  F07[C-F]  F07[C-Br]  \\\n",
       "0      0.0  ...       0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "1      0.0  ...       0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "2      0.0  ...       0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "3      0.0  ...       0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "4      0.0  ...       0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "..     ...  ...       ...       ...       ...       ...       ...        ...   \n",
       "564    0.0  ...       0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "565    0.0  ...       2.0       0.0       0.0       0.0       0.0        0.0   \n",
       "566    0.0  ...       0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "567    0.0  ...       0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "568    0.0  ...       0.0       0.0       0.0       0.0       0.0        0.0   \n",
       "\n",
       "     F09[C-F]  F09[N-F]  F10[C-F]  Class  \n",
       "0         0.0       0.0       0.0      0  \n",
       "1         0.0       0.0       0.0      1  \n",
       "2         0.0       0.0       0.0      2  \n",
       "3         0.0       0.0       0.0      2  \n",
       "4         0.0       0.0       0.0      2  \n",
       "..        ...       ...       ...    ...  \n",
       "564       0.0       0.0       0.0      2  \n",
       "565       0.0       0.0       0.0      1  \n",
       "566       0.0       0.0       0.0      0  \n",
       "567       0.0       0.0       0.0      2  \n",
       "568       0.0       0.0       0.0      0  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trf=one_df.drop('Class',axis=1)\n",
    "y_trf=one_df['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trf=new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpDiam_AEA(bo)    0.172394\n",
       "Chi1_EA(dm)       0.177936\n",
       "GATS2i            0.178341\n",
       "SssCH2            0.179395\n",
       "MCD               0.180083\n",
       "GATS2p            0.181418\n",
       "CATS2D_04_DD      0.200955\n",
       "F03[N-N]          0.216606\n",
       "CATS2D_04_DA      0.251465\n",
       "Class             1.000000\n",
       "Name: Class, dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.corr(method='kendall')['Class'].sort_values().tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2_values, p_values = chi2(x_trf, y_trf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2_df = pd.DataFrame({\n",
    "    'Feature': x_trf.columns,\n",
    "    'Chi2 Value': chi2_values,\n",
    "    'P-Value': p_values\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=chi2_df['Chi2 Value'].sort_values(ascending=False).head(20).index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "632    12656.383169\n",
       "638    11098.930386\n",
       "676     7946.475358\n",
       "512     7063.807070\n",
       "515     6190.267200\n",
       "714     5883.768229\n",
       "566     4914.718082\n",
       "16      4699.042697\n",
       "685     4581.504013\n",
       "565     4474.813309\n",
       "20      4284.200786\n",
       "513     3680.769950\n",
       "534     3626.381428\n",
       "662     3398.076431\n",
       "558     3371.338159\n",
       "526     3345.193075\n",
       "502     2966.163649\n",
       "687     2902.277292\n",
       "652     2841.875707\n",
       "503     2772.627138\n",
       "Name: Chi2 Value, dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chi2_df['Chi2 Value'].sort_values(ascending=False).head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask1=chi2_df['P-Value']<=0.05\n",
    "mask2=chi2_df['Chi2 Value']>=9.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "j=chi2_df[mask1&mask2]['Feature'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MW',\n",
       " 'AMW',\n",
       " 'Sv',\n",
       " 'Mv',\n",
       " 'Me',\n",
       " 'Mp',\n",
       " 'Mi',\n",
       " 'GD',\n",
       " 'nTA',\n",
       " 'nBM',\n",
       " 'RBN',\n",
       " 'RBF',\n",
       " 'nDB',\n",
       " 'nAB',\n",
       " 'nN',\n",
       " 'nO',\n",
       " 'nF',\n",
       " 'nCL',\n",
       " 'nHM',\n",
       " 'nHet',\n",
       " 'nX',\n",
       " 'H%',\n",
       " 'C%',\n",
       " 'N%',\n",
       " 'O%',\n",
       " 'nCsp3',\n",
       " 'nCsp2',\n",
       " 'nCIC',\n",
       " 'nCIR',\n",
       " 'Rbrid',\n",
       " 'MCD',\n",
       " 'RFD',\n",
       " 'NRS',\n",
       " 'NNRS',\n",
       " 'nR05',\n",
       " 'nR06',\n",
       " 'nR09',\n",
       " 'nR10',\n",
       " 'nBnz',\n",
       " 'ARR',\n",
       " 'D/Dtr05',\n",
       " 'D/Dtr06',\n",
       " 'D/Dtr09',\n",
       " 'D/Dtr10',\n",
       " 'D/Dtr11',\n",
       " 'ZM1V',\n",
       " 'ZM1Mad',\n",
       " 'Qindex',\n",
       " 'DBI',\n",
       " 'HNar',\n",
       " 'Xt',\n",
       " 'Ram',\n",
       " 'BLI',\n",
       " 'MSD',\n",
       " 'SPI',\n",
       " 'PJI2',\n",
       " 'CENT',\n",
       " 'ICR',\n",
       " 'Wap',\n",
       " 'PW2',\n",
       " 'PW3',\n",
       " 'PW4',\n",
       " 'PW5',\n",
       " 'MAXDN',\n",
       " 'MAXDP',\n",
       " 'DELS',\n",
       " 'TIE',\n",
       " 'Psi_i_A',\n",
       " 'Psi_i_t',\n",
       " 'BAC',\n",
       " 'LOC',\n",
       " 'SRW05',\n",
       " 'piID',\n",
       " 'PCR',\n",
       " 'PCD',\n",
       " 'X0A',\n",
       " 'X1A',\n",
       " 'X2A',\n",
       " 'X3A',\n",
       " 'X4A',\n",
       " 'X5A',\n",
       " 'X0Av',\n",
       " 'X3Av',\n",
       " 'AAC',\n",
       " 'IDDE',\n",
       " 'IVDE',\n",
       " 'Ges',\n",
       " 'rGes',\n",
       " 'Vindex',\n",
       " 'IC1',\n",
       " 'SIC0',\n",
       " 'SIC1',\n",
       " 'SIC2',\n",
       " 'SIC3',\n",
       " 'CIC0',\n",
       " 'CIC1',\n",
       " 'CIC2',\n",
       " 'CIC3',\n",
       " 'SpPosA_A',\n",
       " 'SpMax_A',\n",
       " 'VE1_A',\n",
       " 'VE2_A',\n",
       " 'VE1sign_A',\n",
       " 'VE2sign_A',\n",
       " 'VE3sign_A',\n",
       " 'VR2_A',\n",
       " 'Chi_D',\n",
       " 'ChiA_D',\n",
       " 'J_D',\n",
       " 'EE_D',\n",
       " 'VE2_D',\n",
       " 'VE1sign_D',\n",
       " 'VE2sign_D',\n",
       " 'VE3sign_D',\n",
       " 'SpMax_L',\n",
       " 'SpMAD_L',\n",
       " 'AVS_X',\n",
       " 'ChiA_X',\n",
       " 'SpPosA_X',\n",
       " 'SM5_X',\n",
       " 'VE1_X',\n",
       " 'VE2_X',\n",
       " 'VE1sign_X',\n",
       " 'VE2sign_X',\n",
       " 'VE3sign_X',\n",
       " 'ChiA_H2',\n",
       " 'SpPosA_H2',\n",
       " 'VE1_H2',\n",
       " 'VE3sign_H2',\n",
       " 'WiA_Dt',\n",
       " 'Chi_Dt',\n",
       " 'ChiA_Dt',\n",
       " 'J_Dt',\n",
       " 'EE_Dt',\n",
       " 'VE1sign_Dt',\n",
       " 'VE2sign_Dt',\n",
       " 'VE3sign_Dt',\n",
       " 'WiA_D/Dt',\n",
       " 'Chi_D/Dt',\n",
       " 'ChiA_D/Dt',\n",
       " 'J_D/Dt',\n",
       " 'VE1_D/Dt',\n",
       " 'VE2_D/Dt',\n",
       " 'VE1sign_D/Dt',\n",
       " 'VE2sign_D/Dt',\n",
       " 'VE3sign_D/Dt',\n",
       " 'SM1_Dz(Z)',\n",
       " 'VE1sign_Dz(Z)',\n",
       " 'VE2sign_Dz(Z)',\n",
       " 'VE3sign_Dz(Z)',\n",
       " 'Chi_Dz(v)',\n",
       " 'J_Dz(v)',\n",
       " 'HyWi_Dz(v)',\n",
       " 'VE2_Dz(v)',\n",
       " 'VE1sign_Dz(v)',\n",
       " 'VE2sign_Dz(v)',\n",
       " 'VE3sign_Dz(v)',\n",
       " 'SM1_Dz(e)',\n",
       " 'J_Dz(p)',\n",
       " 'EE_Dz(p)',\n",
       " 'SM1_Dz(p)',\n",
       " 'VE1sign_Dz(p)',\n",
       " 'VE2sign_Dz(p)',\n",
       " 'VE3sign_Dz(p)',\n",
       " 'VE3sign_Dz(i)',\n",
       " 'AVS_B(m)',\n",
       " 'ChiA_B(m)',\n",
       " 'SpPosA_B(m)',\n",
       " 'SpMax_B(m)',\n",
       " 'SpMaxA_B(m)',\n",
       " 'SpMAD_B(m)',\n",
       " 'EE_B(m)',\n",
       " 'SM3_B(m)',\n",
       " 'VE1_B(m)',\n",
       " 'VE2_B(m)',\n",
       " 'VE1sign_B(m)',\n",
       " 'VE2sign_B(m)',\n",
       " 'VE3sign_B(m)',\n",
       " 'WiA_B(v)',\n",
       " 'AVS_B(v)',\n",
       " 'HyWi_B(v)',\n",
       " 'VE1_B(v)',\n",
       " 'VE2_B(v)',\n",
       " 'VE1sign_B(v)',\n",
       " 'VE2sign_B(v)',\n",
       " 'VE3sign_B(v)',\n",
       " 'AVS_B(e)',\n",
       " 'SpPosA_B(e)',\n",
       " 'SpMax_B(e)',\n",
       " 'SpDiam_B(e)',\n",
       " 'SpMAD_B(e)',\n",
       " 'VE1sign_B(e)',\n",
       " 'AVS_B(p)',\n",
       " 'ChiA_B(p)',\n",
       " 'SpMax_B(p)',\n",
       " 'VE1_B(p)',\n",
       " 'VE1sign_B(p)',\n",
       " 'VE3sign_B(p)',\n",
       " 'SpPosA_B(i)',\n",
       " 'VE3sign_B(i)',\n",
       " 'ChiA_B(s)',\n",
       " 'HyWi_B(s)',\n",
       " 'SpMax_B(s)',\n",
       " 'SpMaxA_B(s)',\n",
       " 'EE_B(s)',\n",
       " 'VE1_B(s)',\n",
       " 'VE1sign_B(s)',\n",
       " 'VE2sign_B(s)',\n",
       " 'VE3sign_B(s)',\n",
       " 'ATS7s',\n",
       " 'ATSC1e',\n",
       " 'ATSC2e',\n",
       " 'ATSC3e',\n",
       " 'ATSC4e',\n",
       " 'ATSC5e',\n",
       " 'ATSC7e',\n",
       " 'ATSC8e',\n",
       " 'ATSC1s',\n",
       " 'ATSC3s',\n",
       " 'ATSC4s',\n",
       " 'MATS1m',\n",
       " 'MATS2m',\n",
       " 'MATS3m',\n",
       " 'MATS4m',\n",
       " 'MATS5m',\n",
       " 'MATS6m',\n",
       " 'MATS7m',\n",
       " 'MATS8m',\n",
       " 'MATS1v',\n",
       " 'MATS2v',\n",
       " 'MATS3v',\n",
       " 'MATS4v',\n",
       " 'MATS6v',\n",
       " 'MATS7v',\n",
       " 'MATS8v',\n",
       " 'MATS1e',\n",
       " 'MATS2e',\n",
       " 'MATS3e',\n",
       " 'MATS4e',\n",
       " 'MATS5e',\n",
       " 'MATS6e',\n",
       " 'MATS7e',\n",
       " 'MATS8e',\n",
       " 'MATS1p',\n",
       " 'MATS2p',\n",
       " 'MATS3p',\n",
       " 'MATS4p',\n",
       " 'MATS5p',\n",
       " 'MATS6p',\n",
       " 'MATS7p',\n",
       " 'MATS1i',\n",
       " 'MATS2i',\n",
       " 'MATS4i',\n",
       " 'MATS5i',\n",
       " 'MATS6i',\n",
       " 'MATS7i',\n",
       " 'MATS8i',\n",
       " 'MATS1s',\n",
       " 'MATS2s',\n",
       " 'MATS3s',\n",
       " 'MATS4s',\n",
       " 'MATS5s',\n",
       " 'MATS6s',\n",
       " 'MATS8s',\n",
       " 'GATS1m',\n",
       " 'GATS2m',\n",
       " 'GATS3m',\n",
       " 'GATS4m',\n",
       " 'GATS5m',\n",
       " 'GATS7m',\n",
       " 'GATS8m',\n",
       " 'GATS1v',\n",
       " 'GATS2v',\n",
       " 'GATS3v',\n",
       " 'GATS4v',\n",
       " 'GATS5v',\n",
       " 'GATS6v',\n",
       " 'GATS7v',\n",
       " 'GATS8v',\n",
       " 'GATS1e',\n",
       " 'GATS2e',\n",
       " 'GATS3e',\n",
       " 'GATS4e',\n",
       " 'GATS5e',\n",
       " 'GATS6e',\n",
       " 'GATS7e',\n",
       " 'GATS8e',\n",
       " 'GATS1p',\n",
       " 'GATS2p',\n",
       " 'GATS3p',\n",
       " 'GATS4p',\n",
       " 'GATS5p',\n",
       " 'GATS6p',\n",
       " 'GATS8p',\n",
       " 'GATS1i',\n",
       " 'GATS2i',\n",
       " 'GATS3i',\n",
       " 'GATS4i',\n",
       " 'GATS5i',\n",
       " 'GATS6i',\n",
       " 'GATS7i',\n",
       " 'GATS8i',\n",
       " 'GATS1s',\n",
       " 'GATS2s',\n",
       " 'GATS3s',\n",
       " 'GATS4s',\n",
       " 'GATS5s',\n",
       " 'GATS6s',\n",
       " 'GATS7s',\n",
       " 'GGI1',\n",
       " 'GGI2',\n",
       " 'GGI3',\n",
       " 'GGI4',\n",
       " 'GGI5',\n",
       " 'GGI7',\n",
       " 'GGI8',\n",
       " 'GGI9',\n",
       " 'GGI10',\n",
       " 'JGI1',\n",
       " 'JGI2',\n",
       " 'JGI3',\n",
       " 'JGI4',\n",
       " 'JGI5',\n",
       " 'JGI6',\n",
       " 'JGI7',\n",
       " 'JGI8',\n",
       " 'JGI9',\n",
       " 'JGI10',\n",
       " 'JGT',\n",
       " 'SpMax2_Bh(m)',\n",
       " 'SpMax3_Bh(m)',\n",
       " 'SpMax4_Bh(m)',\n",
       " 'SpMax5_Bh(m)',\n",
       " 'SpMax1_Bh(e)',\n",
       " 'SpMax2_Bh(e)',\n",
       " 'SpMax3_Bh(e)',\n",
       " 'SpMax4_Bh(e)',\n",
       " 'SpMax1_Bh(p)',\n",
       " 'SpMax2_Bh(p)',\n",
       " 'SpMax2_Bh(s)',\n",
       " 'SpMax4_Bh(s)',\n",
       " 'SpMax5_Bh(s)',\n",
       " 'SpMax6_Bh(s)',\n",
       " 'SpMax7_Bh(s)',\n",
       " 'SpMax8_Bh(s)',\n",
       " 'SpMin1_Bh(m)',\n",
       " 'SpMin2_Bh(m)',\n",
       " 'SpMin3_Bh(m)',\n",
       " 'SpMin4_Bh(m)',\n",
       " 'SpMin5_Bh(m)',\n",
       " 'SpMin6_Bh(m)',\n",
       " 'SpMin7_Bh(m)',\n",
       " 'SpMin4_Bh(v)',\n",
       " 'SpMin5_Bh(v)',\n",
       " 'SpMin1_Bh(e)',\n",
       " 'SpMin2_Bh(e)',\n",
       " 'SpMin3_Bh(e)',\n",
       " 'SpMin1_Bh(p)',\n",
       " 'SpMin2_Bh(p)',\n",
       " 'SpMin1_Bh(s)',\n",
       " 'SpMin2_Bh(s)',\n",
       " 'SpMin3_Bh(s)',\n",
       " 'SpMin6_Bh(s)',\n",
       " 'SpMin7_Bh(s)',\n",
       " 'SpMin8_Bh(s)',\n",
       " 'P_VSA_LogP_1',\n",
       " 'P_VSA_LogP_2',\n",
       " 'P_VSA_LogP_3',\n",
       " 'P_VSA_LogP_4',\n",
       " 'P_VSA_LogP_5',\n",
       " 'P_VSA_LogP_6',\n",
       " 'P_VSA_LogP_8',\n",
       " 'P_VSA_MR_2',\n",
       " 'P_VSA_MR_3',\n",
       " 'P_VSA_MR_5',\n",
       " 'P_VSA_MR_6',\n",
       " 'P_VSA_MR_7',\n",
       " 'P_VSA_MR_8',\n",
       " 'P_VSA_m_2',\n",
       " 'P_VSA_m_3',\n",
       " 'P_VSA_m_4',\n",
       " 'P_VSA_v_2',\n",
       " 'P_VSA_v_3',\n",
       " 'P_VSA_e_2',\n",
       " 'P_VSA_e_3',\n",
       " 'P_VSA_i_1',\n",
       " 'P_VSA_i_2',\n",
       " 'P_VSA_i_3',\n",
       " 'P_VSA_i_4',\n",
       " 'P_VSA_s_1',\n",
       " 'P_VSA_s_4',\n",
       " 'P_VSA_s_5',\n",
       " 'P_VSA_ppp_L',\n",
       " 'P_VSA_ppp_P',\n",
       " 'P_VSA_ppp_N',\n",
       " 'P_VSA_ppp_D',\n",
       " 'P_VSA_ppp_ar',\n",
       " 'P_VSA_ppp_con',\n",
       " 'P_VSA_ppp_hal',\n",
       " 'P_VSA_ppp_cyc',\n",
       " 'Eta_alpha_A',\n",
       " 'Eta_betaS_A',\n",
       " 'Eta_betaP_A',\n",
       " 'Eta_C_A',\n",
       " 'Eta_L_A',\n",
       " 'Eta_F_A',\n",
       " 'Eta_FL_A',\n",
       " 'Eta_B_A',\n",
       " 'Eta_sh_p',\n",
       " 'Eta_sh_y',\n",
       " 'Eta_sh_x',\n",
       " 'SpMAD_EA',\n",
       " 'SpMAD_EA(bo)',\n",
       " 'SpMax_EA(dm)',\n",
       " 'SpMaxA_EA(dm)',\n",
       " 'SpAD_EA(dm)',\n",
       " 'SpMAD_EA(dm)',\n",
       " 'SpMax_EA(ri)',\n",
       " 'SpMAD_EA(ri)',\n",
       " 'SpDiam_AEA(ed)',\n",
       " 'SpMAD_AEA(ed)',\n",
       " 'SpMax_AEA(bo)',\n",
       " 'SpDiam_AEA(bo)',\n",
       " 'SpMAD_AEA(bo)',\n",
       " 'SpMax_AEA(dm)',\n",
       " 'SpMaxA_AEA(dm)',\n",
       " 'SpMAD_AEA(dm)',\n",
       " 'Chi0_EA(dm)',\n",
       " 'Chi1_EA(dm)',\n",
       " 'SM03_EA(bo)',\n",
       " 'SM02_EA(dm)',\n",
       " 'SM03_EA(dm)',\n",
       " 'SM10_AEA(bo)',\n",
       " 'SM11_AEA(bo)',\n",
       " 'SM12_AEA(bo)',\n",
       " 'SM13_AEA(bo)',\n",
       " 'SM14_AEA(bo)',\n",
       " 'SM15_AEA(bo)',\n",
       " 'SM02_AEA(dm)',\n",
       " 'SM03_AEA(dm)',\n",
       " 'SM04_AEA(dm)',\n",
       " 'SM12_AEA(dm)',\n",
       " 'SM13_AEA(dm)',\n",
       " 'SM12_AEA(ri)',\n",
       " 'SM13_AEA(ri)',\n",
       " 'SM14_AEA(ri)',\n",
       " 'SM15_AEA(ri)',\n",
       " 'Eig06_EA(bo)',\n",
       " 'Eig02_EA(dm)',\n",
       " 'Eig03_EA(dm)',\n",
       " 'Eig04_EA(dm)',\n",
       " 'Eig05_EA(dm)',\n",
       " 'Eig06_EA(dm)',\n",
       " 'Eig08_EA(dm)',\n",
       " 'Eig10_EA(dm)',\n",
       " 'Eig02_EA(ri)',\n",
       " 'Eig02_AEA(bo)',\n",
       " 'Eig02_AEA(dm)',\n",
       " 'Eig03_AEA(dm)',\n",
       " 'Eig04_AEA(dm)',\n",
       " 'Eig05_AEA(dm)',\n",
       " 'Eig06_AEA(dm)',\n",
       " 'Eig07_AEA(dm)',\n",
       " 'Eig08_AEA(dm)',\n",
       " 'Eig09_AEA(dm)',\n",
       " 'nCp',\n",
       " 'nCs',\n",
       " 'nCt',\n",
       " 'nCrs',\n",
       " 'nCbH',\n",
       " 'nCb-',\n",
       " 'nCconj',\n",
       " 'nR=Cs',\n",
       " 'nR=Ct',\n",
       " 'nRCOOR',\n",
       " 'nRCONHR',\n",
       " 'nROH',\n",
       " 'nArOH',\n",
       " 'nOHs',\n",
       " 'nArOR',\n",
       " 'nArX',\n",
       " 'nHDon',\n",
       " 'C-005',\n",
       " 'C-006',\n",
       " 'C-008',\n",
       " 'C-016',\n",
       " 'C-024',\n",
       " 'C-025',\n",
       " 'C-026',\n",
       " 'C-028',\n",
       " 'C-034',\n",
       " 'C-040',\n",
       " 'H-046',\n",
       " 'H-047',\n",
       " 'H-051',\n",
       " 'H-052',\n",
       " 'H-053',\n",
       " 'O-056',\n",
       " 'O-058',\n",
       " 'O-060',\n",
       " 'N-072',\n",
       " 'N-075',\n",
       " 'Cl-089',\n",
       " 'SssCH2',\n",
       " 'SdsCH',\n",
       " 'SaaCH',\n",
       " 'SdssC',\n",
       " 'SaasC',\n",
       " 'SaaaC',\n",
       " 'SssssC',\n",
       " 'SssNH',\n",
       " 'SsssN',\n",
       " 'SdsN',\n",
       " 'StN',\n",
       " 'SaasN',\n",
       " 'SaaNH',\n",
       " 'SsOH',\n",
       " 'SssO',\n",
       " 'SssS',\n",
       " 'SaaS',\n",
       " 'NssCH2',\n",
       " 'NsssCH',\n",
       " 'NdssC',\n",
       " 'NaasC',\n",
       " 'CATS2D_03_DD',\n",
       " 'CATS2D_04_DD',\n",
       " 'CATS2D_06_DD',\n",
       " 'CATS2D_07_DD',\n",
       " 'CATS2D_08_DD',\n",
       " 'CATS2D_09_DD',\n",
       " 'CATS2D_02_DA',\n",
       " 'CATS2D_03_DA',\n",
       " 'CATS2D_04_DA',\n",
       " 'CATS2D_05_DA',\n",
       " 'CATS2D_06_DA',\n",
       " 'CATS2D_07_DA',\n",
       " 'CATS2D_08_DA',\n",
       " 'CATS2D_09_DA',\n",
       " 'CATS2D_05_DN',\n",
       " 'CATS2D_02_DL',\n",
       " 'CATS2D_03_DL',\n",
       " 'CATS2D_04_DL',\n",
       " 'CATS2D_05_DL',\n",
       " 'CATS2D_06_DL',\n",
       " 'CATS2D_07_DL',\n",
       " 'CATS2D_08_DL',\n",
       " 'CATS2D_09_DL',\n",
       " 'CATS2D_00_AA',\n",
       " 'CATS2D_01_AA',\n",
       " 'CATS2D_02_AA',\n",
       " 'CATS2D_03_AA',\n",
       " 'CATS2D_04_AA',\n",
       " 'CATS2D_05_AA',\n",
       " 'CATS2D_06_AA',\n",
       " 'CATS2D_07_AA',\n",
       " 'CATS2D_08_AA',\n",
       " 'CATS2D_09_AA',\n",
       " 'CATS2D_04_AP',\n",
       " 'CATS2D_05_AN',\n",
       " 'CATS2D_02_AL',\n",
       " 'CATS2D_03_AL',\n",
       " 'CATS2D_04_AL',\n",
       " 'CATS2D_05_AL',\n",
       " 'CATS2D_07_AL',\n",
       " 'CATS2D_09_AL',\n",
       " 'CATS2D_02_PL',\n",
       " 'CATS2D_03_PL',\n",
       " 'CATS2D_04_PL',\n",
       " 'CATS2D_06_PL',\n",
       " 'CATS2D_07_PL',\n",
       " 'CATS2D_08_PL',\n",
       " 'CATS2D_09_PL',\n",
       " 'CATS2D_03_NL',\n",
       " 'CATS2D_04_NL',\n",
       " 'CATS2D_05_NL',\n",
       " 'CATS2D_06_NL',\n",
       " 'CATS2D_07_NL',\n",
       " 'CATS2D_08_NL',\n",
       " 'CATS2D_09_NL',\n",
       " 'CATS2D_00_LL',\n",
       " 'CATS2D_01_LL',\n",
       " 'CATS2D_02_LL',\n",
       " 'CATS2D_03_LL',\n",
       " 'CATS2D_04_LL',\n",
       " 'CATS2D_05_LL',\n",
       " 'CATS2D_06_LL',\n",
       " 'CATS2D_07_LL',\n",
       " 'CATS2D_08_LL',\n",
       " 'CATS2D_09_LL',\n",
       " 'T(N..N)',\n",
       " 'T(N..O)',\n",
       " 'T(N..S)',\n",
       " 'T(N..F)',\n",
       " 'T(N..Cl)',\n",
       " 'T(N..Br)',\n",
       " 'T(O..O)',\n",
       " 'T(O..S)',\n",
       " 'T(O..F)',\n",
       " 'T(O..Br)',\n",
       " 'T(O..I)',\n",
       " 'T(S..S)',\n",
       " 'T(S..Cl)',\n",
       " 'T(F..F)',\n",
       " 'T(F..Cl)',\n",
       " 'T(Cl..Cl)',\n",
       " 'F01[C-O]',\n",
       " 'F01[N-O]',\n",
       " 'F02[C-N]',\n",
       " 'F02[C-O]',\n",
       " 'F02[C-S]',\n",
       " 'F02[C-F]',\n",
       " 'F02[N-N]',\n",
       " 'F02[N-O]',\n",
       " 'F02[O-O]',\n",
       " 'F03[C-N]',\n",
       " 'F03[C-O]',\n",
       " 'F03[C-S]',\n",
       " 'F03[C-F]',\n",
       " 'F03[N-N]',\n",
       " 'F03[N-O]',\n",
       " 'F03[O-O]',\n",
       " 'F04[C-N]',\n",
       " 'F04[C-O]',\n",
       " 'F04[C-Cl]',\n",
       " 'F04[N-N]',\n",
       " 'F04[N-O]',\n",
       " 'F04[N-Cl]',\n",
       " 'F04[O-O]',\n",
       " 'F05[C-N]',\n",
       " 'F05[C-O]',\n",
       " 'F05[C-S]',\n",
       " 'F05[C-Cl]',\n",
       " 'F05[N-N]',\n",
       " 'F05[N-O]',\n",
       " 'F05[N-S]',\n",
       " 'F05[N-F]',\n",
       " 'F05[O-O]',\n",
       " 'F05[O-S]',\n",
       " 'F06[C-N]',\n",
       " 'F06[C-S]',\n",
       " 'F06[C-F]',\n",
       " 'F06[C-Cl]',\n",
       " 'F06[N-N]',\n",
       " 'F06[N-O]',\n",
       " 'F06[N-F]',\n",
       " 'F06[O-O]',\n",
       " 'F07[C-N]',\n",
       " 'F07[C-O]',\n",
       " 'F07[C-S]',\n",
       " 'F07[C-F]',\n",
       " 'F07[C-Cl]',\n",
       " 'F07[C-Br]',\n",
       " 'F07[N-N]',\n",
       " 'F07[N-O]',\n",
       " 'F07[N-F]',\n",
       " 'F07[O-O]',\n",
       " 'F08[C-N]',\n",
       " 'F08[C-O]',\n",
       " 'F08[C-S]',\n",
       " 'F08[C-F]',\n",
       " 'F08[C-Cl]',\n",
       " 'F08[N-N]',\n",
       " 'F08[N-O]',\n",
       " 'F08[N-F]',\n",
       " 'F08[N-Cl]',\n",
       " 'F08[O-O]',\n",
       " 'F09[C-N]',\n",
       " 'F09[C-O]',\n",
       " 'F09[C-S]',\n",
       " 'F09[C-F]',\n",
       " 'F09[C-Cl]',\n",
       " 'F09[N-N]',\n",
       " 'F09[N-O]',\n",
       " 'F09[N-F]',\n",
       " 'F09[O-O]',\n",
       " 'F10[C-N]',\n",
       " 'F10[C-O]',\n",
       " 'F10[C-S]',\n",
       " 'F10[C-F]',\n",
       " 'F10[C-Cl]',\n",
       " 'F10[N-N]',\n",
       " 'F10[N-O]',\n",
       " 'F10[N-F]',\n",
       " 'F10[O-O]',\n",
       " 'F10[O-S]',\n",
       " 'F10[O-F]',\n",
       " 'Uc',\n",
       " 'AMR',\n",
       " 'MLOGP',\n",
       " 'MLOGP2',\n",
       " 'ALOGP',\n",
       " 'ALOGP2',\n",
       " 'PDI',\n",
       " 'BLTF96',\n",
       " 'DLS_02',\n",
       " 'DLS_03',\n",
       " 'DLS_04',\n",
       " 'DLS_06',\n",
       " 'DLS_cons',\n",
       " 'LLS_01',\n",
       " 'LLS_02',\n",
       " 'Class']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[632,\n",
       " 638,\n",
       " 676,\n",
       " 512,\n",
       " 515,\n",
       " 714,\n",
       " 566,\n",
       " 16,\n",
       " 685,\n",
       " 565,\n",
       " 20,\n",
       " 513,\n",
       " 534,\n",
       " 662,\n",
       " 558,\n",
       " 526,\n",
       " 502,\n",
       " 687,\n",
       " 652,\n",
       " 503]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=chi2_df[mask1&mask2]['Feature'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df=new_df[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akshat\\AppData\\Local\\Temp\\ipykernel_8672\\695168134.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final_df['Class']=new_df['Class']\n"
     ]
    }
   ],
   "source": [
    "final_df['Class']=new_df['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df=new_df.rank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1=RandomForestClassifier(n_estimators=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst.append(np.mean(cross_val_score(clf1,x_train_trf,y_train_trf,cv=10,scoring='accuracy')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Akshat\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Akshat\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Akshat\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Akshat\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Akshat\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Akshat\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Akshat\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Akshat\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Akshat\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Akshat\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Akshat\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Akshat\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Akshat\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Akshat\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Akshat\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Akshat\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Akshat\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Akshat\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Akshat\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Akshat\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "lst.append(np.mean(cross_val_score(clf2,x_train_trf,y_train_trf,cv=10,scoring='accuracy')))\n",
    "lst.append(np.mean(cross_val_score(clf3,x_train_trf,y_train_trf,cv=10,scoring='accuracy')))\n",
    "lst.append(np.mean(cross_val_score(clf4,x_train_trf,y_train_trf,cv=10,scoring='accuracy')))\n",
    "lst.append(np.mean(cross_val_score(clf5,x_train_trf,y_train_trf,cv=10,scoring='accuracy')))\n",
    "lst.append(np.mean(cross_val_score(clf6,x_train_trf,y_train_trf,cv=10,scoring='accuracy')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8224358974358974,\n",
       " 1.0,\n",
       " 0.6542986425339367,\n",
       " 1.0,\n",
       " 0.6621794871794873,\n",
       " 0.671870286576169]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (457645448.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[52], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    plt.\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "plt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_trf,x_test_trf,y_train_trf,y_test_trf=train_test_split(x_trf,y_trf,random_state=42,test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1.fit(x_train_trf,y_train_trf)\n",
    "y_pred=clf1.predict(x_test_trf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2=GradientBoostingClassifier()\n",
    "clf3=LogisticRegression(max_iter=2000)\n",
    "clf4=AdaBoostClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst.append(np.mean(cross_val_score(clf5,x_train_trf,y_train_trf,cv=10,scoring='accuracy')))\n",
    "lst.append(np.mean(cross_val_score(clf6,x_train_trf,y_train_trf,cv=10,scoring='accuracy')))\n",
    "#lst.append(np.mean(cross_val_score(clf4,x_train_trf,y_train_trf,cv=10,scoring='accuracy')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Akshat\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Akshat\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Akshat\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Akshat\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Akshat\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Akshat\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Akshat\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Akshat\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "lst.append(np.mean(cross_val_score(clf3,x_train_trf,y_train_trf,cv=10,scoring='accuracy')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2=RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3508771929824561"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_pred,y_test_trf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5552318295739348"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cross_val_score(clf2,x_trf,y_trf,cv=10,scoring='accuracy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7246983408748114"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(cross_val_score(clf1,x_train_trf,y_train_trf,cv=10,scoring='accuracy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf5=SVC()\n",
    "clf4=AdaBoostClassifier()\n",
    "clf6=KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(cross_val_score(clf,x_train_trf,y_train_trf,cv=10,scoring='accuracy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MW</th>\n",
       "      <th>AMW</th>\n",
       "      <th>Sv</th>\n",
       "      <th>Mv</th>\n",
       "      <th>Me</th>\n",
       "      <th>Mp</th>\n",
       "      <th>Mi</th>\n",
       "      <th>GD</th>\n",
       "      <th>nTA</th>\n",
       "      <th>nBM</th>\n",
       "      <th>...</th>\n",
       "      <th>ALOGP2</th>\n",
       "      <th>PDI</th>\n",
       "      <th>BLTF96</th>\n",
       "      <th>DLS_02</th>\n",
       "      <th>DLS_03</th>\n",
       "      <th>DLS_04</th>\n",
       "      <th>DLS_06</th>\n",
       "      <th>DLS_cons</th>\n",
       "      <th>LLS_01</th>\n",
       "      <th>LLS_02</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>0.317645</td>\n",
       "      <td>0.281847</td>\n",
       "      <td>0.340455</td>\n",
       "      <td>0.874222</td>\n",
       "      <td>0.518868</td>\n",
       "      <td>0.424069</td>\n",
       "      <td>0.209302</td>\n",
       "      <td>0.152174</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>...</td>\n",
       "      <td>0.116478</td>\n",
       "      <td>0.852547</td>\n",
       "      <td>0.560664</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.746269</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.602410</td>\n",
       "      <td>0.409639</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.396825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.190773</td>\n",
       "      <td>0.216472</td>\n",
       "      <td>0.245000</td>\n",
       "      <td>0.839352</td>\n",
       "      <td>0.358491</td>\n",
       "      <td>0.389685</td>\n",
       "      <td>0.279070</td>\n",
       "      <td>0.243478</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.456522</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076418</td>\n",
       "      <td>0.837355</td>\n",
       "      <td>0.357599</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.759036</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0.498603</td>\n",
       "      <td>0.123408</td>\n",
       "      <td>0.541288</td>\n",
       "      <td>0.767123</td>\n",
       "      <td>0.216981</td>\n",
       "      <td>0.277937</td>\n",
       "      <td>0.534884</td>\n",
       "      <td>0.069565</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>...</td>\n",
       "      <td>0.099408</td>\n",
       "      <td>0.863271</td>\n",
       "      <td>0.269476</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.253731</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.192771</td>\n",
       "      <td>0.397590</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>0.346515</td>\n",
       "      <td>0.095276</td>\n",
       "      <td>0.404848</td>\n",
       "      <td>0.739726</td>\n",
       "      <td>0.216981</td>\n",
       "      <td>0.217765</td>\n",
       "      <td>0.546512</td>\n",
       "      <td>0.126087</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076813</td>\n",
       "      <td>0.815907</td>\n",
       "      <td>0.394636</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.507463</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.602410</td>\n",
       "      <td>0.650602</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.396825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>0.162906</td>\n",
       "      <td>0.144639</td>\n",
       "      <td>0.227727</td>\n",
       "      <td>0.779577</td>\n",
       "      <td>0.226415</td>\n",
       "      <td>0.297994</td>\n",
       "      <td>0.581395</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.369565</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018298</td>\n",
       "      <td>0.904379</td>\n",
       "      <td>0.395913</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.879518</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>0.180027</td>\n",
       "      <td>0.218861</td>\n",
       "      <td>0.234242</td>\n",
       "      <td>0.836862</td>\n",
       "      <td>0.339623</td>\n",
       "      <td>0.386819</td>\n",
       "      <td>0.395349</td>\n",
       "      <td>0.282609</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.413043</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034484</td>\n",
       "      <td>0.939231</td>\n",
       "      <td>0.438059</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.843373</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.809524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.162118</td>\n",
       "      <td>0.185686</td>\n",
       "      <td>0.222879</td>\n",
       "      <td>0.814446</td>\n",
       "      <td>0.301887</td>\n",
       "      <td>0.352436</td>\n",
       "      <td>0.406977</td>\n",
       "      <td>0.286957</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020799</td>\n",
       "      <td>0.854334</td>\n",
       "      <td>0.335888</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.759036</td>\n",
       "      <td>0.17</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>0.181603</td>\n",
       "      <td>0.093595</td>\n",
       "      <td>0.256970</td>\n",
       "      <td>0.754670</td>\n",
       "      <td>0.113208</td>\n",
       "      <td>0.275072</td>\n",
       "      <td>0.569767</td>\n",
       "      <td>0.256522</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085298</td>\n",
       "      <td>0.869526</td>\n",
       "      <td>0.264368</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.746269</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.795181</td>\n",
       "      <td>0.746988</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.809524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>0.147647</td>\n",
       "      <td>0.381281</td>\n",
       "      <td>0.172348</td>\n",
       "      <td>0.858032</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.512894</td>\n",
       "      <td>0.383721</td>\n",
       "      <td>0.417391</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.369565</td>\n",
       "      <td>...</td>\n",
       "      <td>0.072817</td>\n",
       "      <td>0.924933</td>\n",
       "      <td>0.224777</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.710843</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>0.207321</td>\n",
       "      <td>0.214260</td>\n",
       "      <td>0.262424</td>\n",
       "      <td>0.845579</td>\n",
       "      <td>0.311321</td>\n",
       "      <td>0.418338</td>\n",
       "      <td>0.267442</td>\n",
       "      <td>0.239130</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.456522</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066650</td>\n",
       "      <td>0.916890</td>\n",
       "      <td>0.409962</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.843373</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.809524</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114 rows × 737 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           MW       AMW        Sv        Mv        Me        Mp        Mi  \\\n",
       "204  0.317645  0.281847  0.340455  0.874222  0.518868  0.424069  0.209302   \n",
       "70   0.190773  0.216472  0.245000  0.839352  0.358491  0.389685  0.279070   \n",
       "131  0.498603  0.123408  0.541288  0.767123  0.216981  0.277937  0.534884   \n",
       "431  0.346515  0.095276  0.404848  0.739726  0.216981  0.217765  0.546512   \n",
       "540  0.162906  0.144639  0.227727  0.779577  0.226415  0.297994  0.581395   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "486  0.180027  0.218861  0.234242  0.836862  0.339623  0.386819  0.395349   \n",
       "75   0.162118  0.185686  0.222879  0.814446  0.301887  0.352436  0.406977   \n",
       "249  0.181603  0.093595  0.256970  0.754670  0.113208  0.275072  0.569767   \n",
       "238  0.147647  0.381281  0.172348  0.858032  0.245283  0.512894  0.383721   \n",
       "265  0.207321  0.214260  0.262424  0.845579  0.311321  0.418338  0.267442   \n",
       "\n",
       "           GD       nTA       nBM  ...    ALOGP2       PDI    BLTF96  DLS_02  \\\n",
       "204  0.152174  0.444444  0.608696  ...  0.116478  0.852547  0.560664    0.50   \n",
       "70   0.243478  0.277778  0.456522  ...  0.076418  0.837355  0.357599    1.00   \n",
       "131  0.069565  0.444444  0.652174  ...  0.099408  0.863271  0.269476    0.33   \n",
       "431  0.126087  0.444444  0.434783  ...  0.076813  0.815907  0.394636    0.50   \n",
       "540  0.300000  0.055556  0.369565  ...  0.018298  0.904379  0.395913    1.00   \n",
       "..        ...       ...       ...  ...       ...       ...       ...     ...   \n",
       "486  0.282609  0.166667  0.413043  ...  0.034484  0.939231  0.438059    1.00   \n",
       "75   0.286957  0.222222  0.347826  ...  0.020799  0.854334  0.335888    1.00   \n",
       "249  0.256522  0.055556  0.391304  ...  0.085298  0.869526  0.264368    0.83   \n",
       "238  0.417391  0.111111  0.369565  ...  0.072817  0.924933  0.224777    1.00   \n",
       "265  0.239130  0.166667  0.456522  ...  0.066650  0.916890  0.409962    1.00   \n",
       "\n",
       "       DLS_03    DLS_04    DLS_06  DLS_cons  LLS_01    LLS_02  \n",
       "204  0.746269  0.222222  0.602410  0.409639    0.17  0.396825  \n",
       "70   1.000000  0.555556  1.000000  0.759036    0.33  1.000000  \n",
       "131  0.253731  0.777778  0.192771  0.397590    0.00  0.000000  \n",
       "431  0.507463  0.888889  0.602410  0.650602    0.17  0.396825  \n",
       "540  1.000000  0.777778  1.000000  0.879518    0.33  1.000000  \n",
       "..        ...       ...       ...       ...     ...       ...  \n",
       "486  1.000000  0.555556  1.000000  0.843373    0.50  0.809524  \n",
       "75   1.000000  0.555556  1.000000  0.759036    0.17  1.000000  \n",
       "249  0.746269  1.000000  0.795181  0.746988    0.17  0.809524  \n",
       "238  1.000000  0.555556  1.000000  0.710843    0.33  1.000000  \n",
       "265  1.000000  0.555556  1.000000  0.843373    0.50  0.809524  \n",
       "\n",
       "[114 rows x 737 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df=pd.concat([x_train_trf,x_test_trf],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df=new_df['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_df=pd.concat([x_df,y_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_df.to_csv('for_regression1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix,matthews_corrcoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n           0       0.50      0.57      0.53        14\\n           1       0.47      0.54      0.50        13\\n           2       0.96      0.83      0.89        30\\n\\n    accuracy                           0.70        57\\n   macro avg       0.64      0.65      0.64        57\\nweighted avg       0.74      0.70      0.71        57\\n'"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_report(y_pred,y_test_trf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8,  5,  2],\n",
       "       [ 4,  3,  7],\n",
       "       [ 3,  5, 20]], dtype=int64)"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test_trf,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47846978638064225"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matthews_corrcoef(y_test_trf,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Akshat\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Akshat\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Akshat\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Akshat\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Akshat\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Akshat\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Akshat\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Akshat\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Akshat\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Akshat\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "lst.append(np.mean(cross_val_score(voting_clf_hard,x_train_trf,y_train_trf,cv=10,scoring='accuracy')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix,matthews_corrcoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_df=pd.DataFrame(columns=['Mod','Accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_df['Mod']=['RandomForestClassifier','GradientBoostingClassifier','LogisticRegression','AdaBoostClassifier','SVC', 'KNN', '60_mod_ens', '60_rf', 'tuned_lR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting_df['Accuracy']=lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mod</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.822436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.654299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.662179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.671870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>60_mod_ens</td>\n",
       "      <td>0.695400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>60_rf</td>\n",
       "      <td>0.753959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tuned_lR</td>\n",
       "      <td>0.654299</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Mod  Accuracy\n",
       "0      RandomForestClassifier  0.822436\n",
       "1  GradientBoostingClassifier  1.000000\n",
       "2          LogisticRegression  0.654299\n",
       "3          AdaBoostClassifier  1.000000\n",
       "4                         SVC  0.662179\n",
       "5                         KNN  0.671870\n",
       "6                  60_mod_ens  0.695400\n",
       "7                       60_rf  0.753959\n",
       "8                    tuned_lR  0.654299"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plotting_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "# Assuming 'plotting' is a dictionary with 'Mod' and 'Accuracy' columns\n",
    "\n",
    "# Define a color list\n",
    "colors = ['blue', 'green', 'red', 'purple', 'orange','violet']  # Adjust colors and length as needed\n",
    "\n",
    "# Create the bar plot with custom colors\n",
    "plt.bar(x=plotting_df['Mod'], height=plotting_df['Accuracy'], color=colors[:len(plotting_df['Mod'])])\n",
    "\n",
    "# Rotate x-axis labels (optional)\n",
    "plt.xticks(rotation='vertical')  \n",
    "for i, (v, label) in enumerate(zip(plotting_df['Accuracy'], plotting_df['Mod'])):\n",
    "  y_pos = v + 0.01  # Adjust y position for label placement\n",
    "  plt.text(i, y_pos, f\"{v:.2f}\", ha='center')  # Format accuracy with 2 decimal places\n",
    "\n",
    "\n",
    "# Add accuracy labels using plt.text (optional)\n",
    "# ... (code for adding accuracy labels remains the same)\n",
    "\n",
    "plt.xlabel(\"Models\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy by Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_clf_hard = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('SVC', clf5),  # Include the first classifier (Logistic Regression)\n",
    "          # Include the second classifier (Random Forest)\n",
    "        ('LR', clf3),  # Include the third classifier (Naive Bayes)\n",
    "        ('KNN', clf6),\n",
    "        ('RF', clf1)\n",
    "    ],\n",
    "    voting='hard'  # Specify hard voting, where the majority class prediction is chosen\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Akshat\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-13 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-13 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-13 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-13 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-13 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-13 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-13 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-13 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-13 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-13 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-13 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-13 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-13 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-13 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-13 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-13 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-13 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-13 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-13 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-13 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-13 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-13 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-13 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-13 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-13 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-13 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-13 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-13 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-13 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-13 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-13 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-13 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-13 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-13 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-13 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-13 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-13 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-13 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-13 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-13 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-13 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-13 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-13\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>AdaBoostClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" checked><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;AdaBoostClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\">?<span>Documentation for AdaBoostClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>AdaBoostClassifier()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "AdaBoostClassifier()"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=clf4.predict(x_test_trf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5964912280701754"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_pred,y_test_trf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9,  4,  3],\n",
       "       [ 4,  4,  7],\n",
       "       [ 3,  2, 21]], dtype=int64)"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test_trf,y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1   2\n",
       "0  9  4   3\n",
       "1  4  4   7\n",
       "2  3  2  21"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(confusion_matrix(y_test_trf,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n           0       0.56      0.56      0.56        16\\n           1       0.27      0.40      0.32        10\\n           2       0.81      0.68      0.74        31\\n\\n    accuracy                           0.60        57\\n   macro avg       0.55      0.55      0.54        57\\nweighted avg       0.64      0.60      0.61        57\\n'"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_report(y_pred, y_test_trf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
